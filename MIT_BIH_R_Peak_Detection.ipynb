{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIT-BIH R-Peak Detection - Multiple Methods\n",
    "\n",
    "**Author:** Kumar Abhishek  \n",
    "**Date:** October 2025\n",
    "\n",
    "This notebook implements **4 different methods** for R-peak detection:\n",
    "1. Pan-Tompkins Algorithm\n",
    "2. Wavelet Transform Detection\n",
    "3. 1D CNN (Deep Learning)\n",
    "4. LSTM (Deep Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run if packages are not installed\n",
    "# !pip install -r requirements.txt\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import wfdb\n",
    "import scipy.signal as signal\n",
    "import pywt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mitbih_record(record_name='100', sampfrom=0, sampto=None):\n",
    "    \"\"\"Load MIT-BIH record with annotations\"\"\"\n",
    "    record = wfdb.rdrecord(record_name, pn_dir='mitdb', sampfrom=sampfrom, sampto=sampto)\n",
    "    annotation = wfdb.rdann(record_name, 'atr', pn_dir='mitdb', sampfrom=sampfrom, sampto=sampto)\n",
    "    ecg_signal = record.p_signal[:, 0]\n",
    "    fs = record.fs\n",
    "    r_peaks = annotation.sample\n",
    "    return ecg_signal, r_peaks, fs\n",
    "\n",
    "def load_multiple_records(record_list, duration=10000):\n",
    "    \"\"\"Load multiple records for training\"\"\"\n",
    "    all_signals = []\n",
    "    all_peaks = []\n",
    "    for record in record_list:\n",
    "        try:\n",
    "            sig, peaks, fs = load_mitbih_record(record, sampto=duration)\n",
    "            all_signals.append(sig)\n",
    "            all_peaks.append(peaks)\n",
    "            print(f\"Loaded record {record}: {len(sig)} samples, {len(peaks)} peaks\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {record}: {e}\")\n",
    "    return all_signals, all_peaks, fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pan-Tompkins Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PanTompkinsDetector:\n",
    "    def __init__(self, fs=360):\n",
    "        self.fs = fs\n",
    "    \n",
    "    def bandpass_filter(self, signal_data):\n",
    "        from scipy.signal import filtfilt\n",
    "        b_low = np.array([1, 0, 0, 0, 0, 0, -2, 0, 0, 0, 0, 0, 1])\n",
    "        a_low = np.array([1, -2, 1])\n",
    "        b_high = np.array([-1] + [0]*15 + [32, -32] + [0]*14 + [1])\n",
    "        a_high = np.array([1, -1])\n",
    "        filtered = filtfilt(b_low, a_low, signal_data)\n",
    "        filtered = filtfilt(b_high, a_high, filtered)\n",
    "        return filtered\n",
    "    \n",
    "    def detect(self, ecg_signal):\n",
    "        filtered = self.bandpass_filter(ecg_signal)\n",
    "        derivative = np.diff(filtered)\n",
    "        squared = derivative ** 2\n",
    "        window_size = int(0.150 * self.fs)\n",
    "        window = np.ones(window_size) / window_size\n",
    "        integrated = np.convolve(squared, window, mode='same')\n",
    "        \n",
    "        peaks = []\n",
    "        threshold = np.max(integrated) * 0.5\n",
    "        refractory = int(0.2 * self.fs)\n",
    "        \n",
    "        for i in range(1, len(integrated) - 1):\n",
    "            if integrated[i] > threshold and integrated[i] > integrated[i-1] and integrated[i] > integrated[i+1]:\n",
    "                if not peaks or (i - peaks[-1]) > refractory:\n",
    "                    peaks.append(i)\n",
    "        return np.array(peaks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Wavelet-Based Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveletDetector:\n",
    "    def __init__(self, fs=360, wavelet='db4', level=4):\n",
    "        self.fs = fs\n",
    "        self.wavelet = wavelet\n",
    "        self.level = level\n",
    "    \n",
    "    def detect(self, ecg_signal):\n",
    "        coeffs = pywt.swt(ecg_signal, self.wavelet, level=self.level)\n",
    "        detail_coeffs = coeffs[2][1]\n",
    "        squared = detail_coeffs ** 2\n",
    "        window_size = int(0.120 * self.fs)\n",
    "        smoothed = np.convolve(squared, np.ones(window_size)/window_size, mode='same')\n",
    "        threshold = np.mean(smoothed) + 0.5 * np.std(smoothed)\n",
    "        \n",
    "        peaks = []\n",
    "        refractory = int(0.2 * self.fs)\n",
    "        for i in range(1, len(smoothed) - 1):\n",
    "            if smoothed[i] > threshold and smoothed[i] > smoothed[i-1] and smoothed[i] > smoothed[i+1]:\n",
    "                if not peaks or (i - peaks[-1]) > refractory:\n",
    "                    search_window = 20\n",
    "                    start = max(0, i - search_window)\n",
    "                    end = min(len(ecg_signal), i + search_window)\n",
    "                    local_max = start + np.argmax(ecg_signal[start:end])\n",
    "                    peaks.append(local_max)\n",
    "        return np.array(peaks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deep Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(input_shape=(300, 1)):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv1D(32, 5, activation='relu', input_shape=input_shape),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling1D(2),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Conv1D(64, 5, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling1D(2),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Conv1D(128, 3, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling1D(2),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy',\n",
    "                  metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "    return model\n",
    "\n",
    "def create_lstm_model(input_shape=(300, 1)):\n",
    "    model = models.Sequential([\n",
    "        layers.LSTM(64, return_sequences=True, input_shape=input_shape),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.LSTM(32, return_sequences=True),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.LSTM(16),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy',\n",
    "                  metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data(signals, all_peaks, window_size=150, fs=360):\n",
    "    X, y = [], []\n",
    "    for signal_data, peaks in zip(signals, all_peaks):\n",
    "        signal_norm = (signal_data - np.mean(signal_data)) / np.std(signal_data)\n",
    "        for peak in peaks:\n",
    "            start, end = peak - window_size, peak + window_size\n",
    "            if start >= 0 and end < len(signal_norm):\n",
    "                X.append(signal_norm[start:end])\n",
    "                y.append(1)\n",
    "        num_negative = len(peaks)\n",
    "        for _ in range(num_negative):\n",
    "            while True:\n",
    "                idx = np.random.randint(window_size, len(signal_norm) - window_size)\n",
    "                if np.min(np.abs(peaks - idx)) > int(0.3 * fs):\n",
    "                    break\n",
    "            X.append(signal_norm[idx-window_size:idx+window_size])\n",
    "            y.append(0)\n",
    "    return np.array(X).reshape(-1, window_size*2, 1), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(detected_peaks, true_peaks, tolerance=50):\n",
    "    TP, FP, matched_true = 0, 0, set()\n",
    "    for detected in detected_peaks:\n",
    "        distances = np.abs(true_peaks - detected)\n",
    "        min_idx = np.argmin(distances)\n",
    "        if distances[min_idx] <= tolerance and min_idx not in matched_true:\n",
    "            TP += 1\n",
    "            matched_true.add(min_idx)\n",
    "        else:\n",
    "            FP += 1\n",
    "    FN = len(true_peaks) - TP\n",
    "    sens = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    prec = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    f1 = 2 * (prec * sens) / (prec + sens) if (prec + sens) > 0 else 0\n",
    "    return {'TP': TP, 'FP': FP, 'FN': FN, 'Sensitivity': sens*100, \n",
    "            'Precision': prec*100, 'F1-Score': f1*100}\n",
    "\n",
    "def print_metrics(metrics, name):\n",
    "    print(f\"\\n{'='*50}\\n{name}\\n{'='*50}\")\n",
    "    for k, v in metrics.items():\n",
    "        if isinstance(v, float):\n",
    "            print(f\"{k}: {v:.2f}%\")\n",
    "        else:\n",
    "            print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_detection_results(ecg, detected, true, name, fs=360, dur=10):\n",
    "    samples = int(dur * fs)\n",
    "    time = np.arange(samples) / fs\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(time, ecg[:samples], 'k-', linewidth=0.8, label='ECG')\n",
    "    true_in = true[true < samples]\n",
    "    det_in = detected[detected < samples]\n",
    "    plt.scatter(true_in/fs, ecg[true_in], c='green', s=100, marker='o', label='True', zorder=5)\n",
    "    plt.scatter(det_in/fs, ecg[det_in], c='red', s=50, marker='x', label='Detected', zorder=4)\n",
    "    plt.xlabel('Time (s)'); plt.ylabel('Amplitude')\n",
    "    plt.title(f'{name}', fontweight='bold')\n",
    "    plt.legend(); plt.grid(alpha=0.3); plt.tight_layout(); plt.show()\n",
    "\n",
    "def plot_comparison(ecg, methods, true, fs=360, dur=5):\n",
    "    samples = int(dur * fs)\n",
    "    time = np.arange(samples) / fs\n",
    "    fig, axes = plt.subplots(len(methods)+1, 1, figsize=(15, 3*(len(methods)+1)))\n",
    "    axes[0].plot(time, ecg[:samples], 'k-', linewidth=0.8)\n",
    "    true_in = true[true < samples]\n",
    "    axes[0].scatter(true_in/fs, ecg[true_in], c='green', s=100, label='True')\n",
    "    axes[0].set_title('Ground Truth', fontweight='bold')\n",
    "    axes[0].legend(); axes[0].grid(alpha=0.3)\n",
    "    for i, (name, det) in enumerate(methods.items(), 1):\n",
    "        axes[i].plot(time, ecg[:samples], 'k-', alpha=0.5, linewidth=0.8)\n",
    "        det_in = det[det < samples]\n",
    "        axes[i].scatter(det_in/fs, ecg[det_in], c='red', s=50, marker='x', label='Detected')\n",
    "        axes[i].scatter(true_in/fs, ecg[true_in], c='green', s=30, alpha=0.3)\n",
    "        axes[i].set_title(name, fontweight='bold')\n",
    "        axes[i].legend(); axes[i].grid(alpha=0.3)\n",
    "    axes[-1].set_xlabel('Time (s)')\n",
    "    plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Main Execution Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"MIT-BIH R-PEAK DETECTION - STARTING...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load test data\n",
    "print(\"\\n[1/6] Loading test data...\")\n",
    "ecg_signal, true_peaks, fs = load_mitbih_record('100', sampto=650000)\n",
    "print(f\"Loaded: {len(ecg_signal)} samples, {len(true_peaks)} peaks, {fs}Hz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test Classical Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pan-Tompkins\n",
    "print(\"\\n[2/6] Testing Pan-Tompkins...\")\n",
    "pt_detector = PanTompkinsDetector(fs=fs)\n",
    "pt_peaks = pt_detector.detect(ecg_signal)\n",
    "pt_metrics = calculate_metrics(pt_peaks, true_peaks)\n",
    "print_metrics(pt_metrics, \"Pan-Tompkins\")\n",
    "\n",
    "# Wavelet\n",
    "print(\"\\n[3/6] Testing Wavelet...\")\n",
    "wt_detector = WaveletDetector(fs=fs)\n",
    "wt_peaks = wt_detector.detect(ecg_signal)\n",
    "wt_metrics = calculate_metrics(wt_peaks, true_peaks)\n",
    "print_metrics(wt_metrics, \"Wavelet Transform\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Prepare Deep Learning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[4/6] Preparing deep learning data...\")\n",
    "train_records = ['100', '101', '103', '105', '106', '108', '109', '111', '112', '113']\n",
    "train_signals, train_peaks, _ = load_multiple_records(train_records, duration=50000)\n",
    "X, y = create_training_data(train_signals, train_peaks, window_size=150, fs=fs)\n",
    "print(f\"Dataset: {X.shape[0]} samples ({np.sum(y)} positive, {len(y)-np.sum(y)} negative)\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(f\"Train: {len(X_train)} | Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Train CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[5/6] Training CNN...\")\n",
    "cnn_model = create_cnn_model(input_shape=(X.shape[1], 1))\n",
    "history_cnn = cnn_model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "                             epochs=20, batch_size=64, verbose=1)\n",
    "cnn_results = cnn_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nCNN Results: Acc={cnn_results[1]*100:.2f}%, Prec={cnn_results[2]*100:.2f}%, Rec={cnn_results[3]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Train LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[6/6] Training LSTM...\")\n",
    "lstm_model = create_lstm_model(input_shape=(X.shape[1], 1))\n",
    "history_lstm = lstm_model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "                               epochs=20, batch_size=64, verbose=1)\n",
    "lstm_results = lstm_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nLSTM Results: Acc={lstm_results[1]*100:.2f}%, Prec={lstm_results[2]*100:.2f}%, Rec={lstm_results[3]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot individual results\n",
    "plot_detection_results(ecg_signal, pt_peaks, true_peaks, \"Pan-Tompkins\", fs, 10)\n",
    "plot_detection_results(ecg_signal, wt_peaks, true_peaks, \"Wavelet Transform\", fs, 10)\n",
    "\n",
    "# Comparison plot\n",
    "methods_results = {'Pan-Tompkins': pt_peaks, 'Wavelet': wt_peaks}\n",
    "plot_comparison(ecg_signal, methods_results, true_peaks, fs, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Training History Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes[0,0].plot(history_cnn.history['loss'], label='Train')\n",
    "axes[0,0].plot(history_cnn.history['val_loss'], label='Val')\n",
    "axes[0,0].set_title('CNN - Loss', fontweight='bold')\n",
    "axes[0,0].legend(); axes[0,0].grid(alpha=0.3)\n",
    "\n",
    "axes[0,1].plot(history_cnn.history['accuracy'], label='Train')\n",
    "axes[0,1].plot(history_cnn.history['val_accuracy'], label='Val')\n",
    "axes[0,1].set_title('CNN - Accuracy', fontweight='bold')\n",
    "axes[0,1].legend(); axes[0,1].grid(alpha=0.3)\n",
    "\n",
    "axes[1,0].plot(history_lstm.history['loss'], label='Train')\n",
    "axes[1,0].plot(history_lstm.history['val_loss'], label='Val')\n",
    "axes[1,0].set_title('LSTM - Loss', fontweight='bold')\n",
    "axes[1,0].legend(); axes[1,0].grid(alpha=0.3)\n",
    "\n",
    "axes[1,1].plot(history_lstm.history['accuracy'], label='Train')\n",
    "axes[1,1].plot(history_lstm.history['val_accuracy'], label='Val')\n",
    "axes[1,1].set_title('LSTM - Accuracy', fontweight='bold')\n",
    "axes[1,1].legend(); axes[1,1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.save('rpeak_cnn_model.h5')\n",
    "lstm_model.save('rpeak_lstm_model.h5')\n",
    "print(\"Models saved successfully!\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ALL PROCESSES COMPLETED!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFINAL SUMMARY:\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nClassical Methods:\")\n",
    "print(f\"  Pan-Tompkins: F1={pt_metrics['F1-Score']:.2f}%, Sens={pt_metrics['Sensitivity']:.2f}%\")\n",
    "print(f\"  Wavelet:      F1={wt_metrics['F1-Score']:.2f}%, Sens={wt_metrics['Sensitivity']:.2f}%\")\n",
    "print(\"\\nDeep Learning Models:\")\n",
    "print(f\"  CNN:  Acc={cnn_results[1]*100:.2f}%, Prec={cnn_results[2]*100:.2f}%, Rec={cnn_results[3]*100:.2f}%\")\n",
    "print(f\"  LSTM: Acc={lstm_results[1]*100:.2f}%, Prec={lstm_results[2]*100:.2f}%, Rec={lstm_results[3]*100:.2f}%\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
